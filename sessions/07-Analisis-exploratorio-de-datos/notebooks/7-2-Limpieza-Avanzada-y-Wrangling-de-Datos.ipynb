{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95ae080f",
      "metadata": {},
      "source": [
        "# Limpieza Avanzada y Wrangling de Datos\n",
        "\n",
        "**Curso:** Fundamentos de Programación y Analítica de Datos con Python  \n",
        "**Duración estimada del bloque:** 2 horas  \n",
        "**Autores:** Jhon Erik Navarrete Gómez, Equipo de Instrucción – Curso Python (Asesoría)  \n",
        "**Creado:** 2025-09-16T18:31:24+00:00\n",
        "\n",
        "**Descripción breve del bloque:**  \n",
        "Bloque orientado a asegurar la calidad de los datos mediante técnicas de manejo de valores faltantes, detección y tratamiento de outliers, transformación y estandarización de variables, y construcción de pipelines de limpieza reproducibles con pandas y scikit-learn.\n",
        "\n",
        "## Objetivos específicos\n",
        "- Diagnosticar y tratar valores faltantes con estrategias apropiadas por tipo de variable y contexto.\n",
        "- Detectar y manejar outliers mediante reglas basadas en IQR y criterios robustos para reducir su impacto.\n",
        "- Convertir y validar tipos de datos (fechas, categorías), crear variables derivadas y normalizar/estandarizar.\n",
        "- Construir pipelines reproducibles de limpieza con `scikit-learn` para separar preparación y modelado.\n",
        "- Documentar supuestos y decisiones de calidad de datos para auditoría y reproducibilidad.\n",
        "\n",
        "## Prerrequisitos\n",
        "- Conocimientos básicos de Python y `pandas`.\n",
        "- Familiaridad con el EDA inicial del bloque 7.1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9218617b",
      "metadata": {},
      "source": [
        "## Preparación del entorno y dataset base\n",
        "\n",
        "En esta sección se cargará el dataset de trabajo (Titanic si está disponible; en su defecto se generará un DataFrame sintético).\n",
        "Se definen utilidades de diagnóstico para reutilizar en los ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d17728",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Carga del dataset (Titanic si está disponible) o uno sintético\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "\n",
        "df = None\n",
        "dataset_source = None\n",
        "try:\n",
        "    import seaborn as sns  # opcional; si no está, se usará un dataset sintético\n",
        "    df = sns.load_dataset(\"titanic\")\n",
        "    dataset_source = \"seaborn.load_dataset('titanic')\"\n",
        "except Exception:\n",
        "    df = None\n",
        "\n",
        "if df is None:\n",
        "    data = {\n",
        "        \"survived\": [0, 1, 1, 0, 1, 0, 0, 1, 0, 1],\n",
        "        \"pclass\":   [3, 1, 3, 2, 1, 3, 2, 1, 2, 3],\n",
        "        \"sex\":      [\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\"],\n",
        "        \"age\":      [22.0, 38.0, 26.0, 35.0, None, 28.0, 42.0, 19.0, 54.0, None],\n",
        "        \"sibsp\":    [1, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
        "        \"parch\":    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "        \"fare\":     [7.25, 71.28, 7.92, 8.05, 53.10, 8.46, 13.0, 26.55, 51.86, 7.75],\n",
        "        \"embarked\": [\"S\",\"C\",\"S\",\"S\",\"S\",\"Q\",\"S\",\"C\",\"S\",\"Q\"],\n",
        "        \"deck\":     [None, \"C\", None, None, \"B\", None, None, \"C\", \"D\", None],\n",
        "        \"who\":      [\"man\",\"woman\",\"woman\",\"man\",\"woman\",\"man\",\"man\",\"woman\",\"woman\",\"man\"],\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    dataset_source = \"DataFrame sintético (estructura Titanic-like)\"\n",
        "\n",
        "print(f\"Fuente del dataset: {dataset_source}\")\n",
        "display(df.head())\n",
        "print(\"\\\\nInfo:\")\n",
        "print(df.info())\n",
        "print(\"\\\\nNulos por columna:\")\n",
        "display(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fadc4d43",
      "metadata": {},
      "source": [
        "## Tema 1 — Manejo de valores faltantes\n",
        "\n",
        "### Definición\n",
        "Los valores faltantes representan ausencia de datos observables. Su tratamiento consiste en identificarlos, cuantificarlos y decidir estrategias de imputación o eliminación con criterios técnicos y de negocio.\n",
        "\n",
        "### Importancia en programación y analítica de datos\n",
        "- Evitan sesgos y errores en métricas, pruebas de hipótesis y modelos.\n",
        "- Permiten usar la mayor cantidad posible de información sin degradar calidad.\n",
        "- Habilitan pipelines reproducibles donde la imputación forma parte del preprocesamiento.\n",
        "\n",
        "### Buenas prácticas profesionales y errores comunes\n",
        "**Buenas prácticas:**\n",
        "- Reportar porcentaje de nulos y justificar la estrategia por variable.\n",
        "- Usar imputación basada en distribución (mediana/moda) para robustez inicial.\n",
        "- Mantener una columna bandera (indicador) cuando la ausencia es informativa.\n",
        "\n",
        "**Errores comunes:**\n",
        "- Imputar sin analizar el mecanismo de ausencia (MCAR, MAR, MNAR).\n",
        "- Mezclar escalas o tipos tras la imputación (por ejemplo, categorías codificadas como números)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12b6708",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Ejemplo — Estrategias básicas de imputación con pandas y SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546975fd",
      "metadata": {},
      "source": [
        "## Tema 2 — Detección y manejo de outliers\n",
        "\n",
        "### Definición\n",
        "Un outlier es una observación que se aleja considerablemente del patrón de la mayoría de los datos. Su detección clásica se basa en la regla de 1.5×IQR o criterios robustos alternativos según el dominio.\n",
        "\n",
        "### Importancia en programación y analítica de datos\n",
        "- Los outliers pueden distorsionar estadísticas (media, varianza) y modelos sensibles.\n",
        "- Decidir si tratarlos, recortarlos o transformarlos depende del contexto de negocio y de la causa.\n",
        "\n",
        "### Buenas prácticas profesionales y errores comunes\n",
        "**Buenas prácticas:**\n",
        "- Visualizar la distribución (histogramas, boxplots) antes y después del tratamiento.\n",
        "- Documentar criterios y justificar impactos en métricas y modelado.\n",
        "\n",
        "**Errores comunes:**\n",
        "- Eliminar outliers sin análisis de causa (error vs valor extremo legítimo).\n",
        "- Aplicar el mismo umbral a todas las variables sin considerar unidades o escalas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f027521",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Ejemplo — Detección por IQR y tratamiento por recorte (clipping)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4ee7f4",
      "metadata": {},
      "source": [
        "## Tema 3 — Wrangling avanzado con pandas\n",
        "\n",
        "### Definición\n",
        "El *data wrangling* es el conjunto de transformaciones para convertir datos crudos en un formato analíticamente útil: cambio de tipos, normalización/estandarización, derivación de variables, operaciones con fechas y categorías, y consolidación de calidad.\n",
        "\n",
        "### Importancia en programación y analítica de datos\n",
        "- Garantiza consistencia semántica y técnica para análisis robustos.\n",
        "- Reduce errores en el modelado al estandarizar formatos y escalas.\n",
        "- Mejora la interpretabilidad al producir variables derivadas con significado.\n",
        "\n",
        "### Buenas prácticas profesionales y errores comunes\n",
        "**Buenas prácticas:**\n",
        "- Mantener funciones puras de transformación y pruebas unitarias para reglas críticas.\n",
        "- Evitar mutaciones no controladas y documentar cada paso.\n",
        "\n",
        "**Errores comunes:**\n",
        "- No validar conversión de fechas o categorías.\n",
        "- Mezclar operaciones de limpieza con experimentos de modelado en la misma celda sin control de versiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de84b46d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Ejemplo — Conversión de tipos, fechas, categorías y variables derivadas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11078d06",
      "metadata": {},
      "source": [
        "## Tema 4 — Pipelines de limpieza con scikit-learn\n",
        "\n",
        "### Definición\n",
        "Un *pipeline* es una secuencia de pasos que transforma datos de forma reproducible y componible. `scikit-learn` provee `Pipeline` y `ColumnTransformer` para encadenar imputación, escalado y codificación, separando la preparación del modelado.\n",
        "\n",
        "### Importancia en programación y analítica de datos\n",
        "- Reproducibilidad y trazabilidad de la preparación de datos.\n",
        "- Facilita validación cruzada sin fugas de información (*data leakage*).\n",
        "- Portabilidad del flujo de preprocesamiento a producción.\n",
        "\n",
        "### Buenas prácticas profesionales y errores comunes\n",
        "**Buenas prácticas:**\n",
        "- Separar columnas numéricas y categóricas y aplicar transformaciones específicas.\n",
        "- Ajustar solo con datos de entrenamiento y aplicar transformaciones a validación/prueba.\n",
        "\n",
        "**Errores comunes:**\n",
        "- Estandarizar antes de imputar o mezclar codificación con variables mal tipadas.\n",
        "- Ajustar el pipeline con datos que incluyen el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cd65ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Ejemplo — Pipeline + ColumnTransformer (imputación y escalado/codificación)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc52b29",
      "metadata": {},
      "source": [
        "# Ejercicios integradores\n",
        "\n",
        "A continuación se presentan ejercicios que abarcan manejo de valores faltantes, outliers, wrangling y pipelines.\n",
        "Cada ejercicio contiene: contexto técnico, datos/entradas, requerimientos, criterios de aceptación y pistas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6c8c04",
      "metadata": {},
      "source": [
        "### Ejercicio 1 — Estrategia de imputación documentada\n",
        "\n",
        "**Contexto técnico:**  \n",
        "Como Ingeniero/a de Datos, debe definir una estrategia de imputación inicial para un flujo de analítica descriptiva en un dashboard operativo.\n",
        "\n",
        "**Datos/entradas:**  \n",
        "Use el `df` del notebook. Seleccione al menos 2 columnas numéricas y 2 categóricas con nulos.\n",
        "\n",
        "**Requerimientos:**  \n",
        "1. Calcule el porcentaje de nulos por columna.  \n",
        "2. Impute numéricas con mediana y categóricas con moda.  \n",
        "3. Cree indicadores binarios de ausencia para las columnas imputadas.  \n",
        "4. Documente supuestos y justificaciones en 4–6 líneas.\n",
        "\n",
        "**Criterios de aceptación:**  \n",
        "- Imputación realizada sin nulos residuales en las columnas tratadas.  \n",
        "- Indicadores de ausencia creados y verificados.\n",
        "\n",
        "**Pistas:**  \n",
        "- `SimpleImputer`, `isna().mean()`, `astype(int)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885f2ad8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Solución Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f0753b",
      "metadata": {},
      "source": [
        "### Ejercicio 2 — Outliers bajo regla IQR y tratamiento\n",
        "\n",
        "**Contexto técnico:**  \n",
        "El equipo de riesgos solicita estabilizar métricas de costo para reducir sensibilidad a extremos en reportes semanales.\n",
        "\n",
        "**Datos/entradas:**  \n",
        "Use una columna numérica con variabilidad (por ejemplo, `fare` si existe).\n",
        "\n",
        "**Requerimientos:**  \n",
        "1. Detecte outliers por 1.5×IQR.  \n",
        "2. Reporte porcentaje de outliers respecto al total.  \n",
        "3. Aplique tratamiento por recorte (clipping) y valide el cambio en mediana y rango intercuartílico.\n",
        "\n",
        "**Criterios de aceptación:**  \n",
        "- Porcentaje y conteos correctos.  \n",
        "- Evidencia de cambio controlado en métricas (mediana e IQR).\n",
        "\n",
        "**Pistas:**  \n",
        "- `quantile`, `clip`, `dropna`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3672cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Solución Ejercicio 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634ad563",
      "metadata": {},
      "source": [
        "### Ejercicio 3 — Wrangling y consistencia de tipos\n",
        "\n",
        "**Contexto técnico:**  \n",
        "Se requiere un conjunto de datos consistente para alimentar un microservicio que calcula indicadores operativos. El contrato exige tipos específicos y variables derivadas.\n",
        "\n",
        "**Datos/entradas:**  \n",
        "Use `df` y cree: conversión de categorías, conversión de fecha, y variable `family_size` (si aplica).\n",
        "\n",
        "**Requerimientos:**  \n",
        "1. Convertir columnas categóricas a tipo `category`.  \n",
        "2. Asegurar columna de fecha válida (`datetime64[ns]`).  \n",
        "3. Crear `family_size = sibsp + parch + 1` si ambas existen.  \n",
        "4. Normalizar una columna numérica a z-score (`(x-μ)/σ`).\n",
        "\n",
        "**Criterios de aceptación:**  \n",
        "- Dtypes correctos verificados con `df.dtypes`.  \n",
        "- Columnas nuevas creadas correctamente.\n",
        "\n",
        "**Pistas:**  \n",
        "- `astype(\"category\")`, `to_datetime`, operaciones aritméticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d4aca7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Solución Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ad616d",
      "metadata": {},
      "source": [
        "### Ejercicio 4 — Pipeline de limpieza reutilizable\n",
        "\n",
        "**Contexto técnico:**  \n",
        "El equipo de ciencia de datos necesita un pipeline uniforme para preprocesar datos antes de entrenar modelos y ejecutar validaciones periódicas.\n",
        "\n",
        "**Datos/entradas:**  \n",
        "Use `df` y defina un `ColumnTransformer` con ramas numérica y categórica.\n",
        "\n",
        "**Requerimientos:**  \n",
        "1. Imputar numéricas con mediana y escalar con `StandardScaler`.  \n",
        "2. Imputar categóricas con moda y codificar con `OneHotEncoder`.  \n",
        "3. Generar `X_prepared` (matriz lista para modelar) y reportar su forma.  \n",
        "4. Guardar el pipeline ajustado con `joblib`.\n",
        "\n",
        "**Criterios de aceptación:**  \n",
        "- Transformación sin errores y dimensiones coherentes.  \n",
        "- Archivo del pipeline guardado en disco.\n",
        "\n",
        "**Pistas:**  \n",
        "- `Pipeline`, `ColumnTransformer`, `joblib.dump`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286f3801",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Solución Ejercicio 4"
      ]
    }
  ],
  "metadata": {
    "authors": [
      "Jhon Erik Navarrete Gómez",
      "Equipo de Instrucción – Curso Python (Asesoría)"
    ],
    "created": "2025-09-16T18:31:24+00:00",
    "description": "Bloque orientado a asegurar la calidad de los datos mediante técnicas de manejo de valores faltantes, detección y tratamiento de outliers, transformación y estandarización de variables, y construcción de pipelines de limpieza reproducibles con pandas y scikit-learn.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
